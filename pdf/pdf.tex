\graphicspath{{./pdf/figures/}}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\usetikzlibrary{shapes.geometric}
\tikzstyle{database} = [cylinder, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=yellow!30, shape border rotate=90, aspect=0.25]

\chapter{Atomic Pair Distribution Function: \\Theory and Computation} \label{ch:pdf}
\section{Theory}
To properly understand the PDF and its limitations we need to derive its mathematics.
The PDF has been previously derived many times so it is not re-derived here.
This discussion of the PDF and its gradients use the notation of Farrow and Billinge. \cite{Farrow2009}
\subsection{Derivation}
Many of the above techniques require the gradient of the PES.
This in turn requires the gradient of the PDF to be derived.
Mathematicly treating thermal vibrations will also be discussed in this seciton.
Systems which are truly extended materials, like powders with particle sizes larger than 10nm, are best formulated as systems with periodic boundaries.
Thus, the equations for a periodicly bound PDF need to be developed as well, with their gradients.
\input{pdf/der}

\section{Computation} \label{sec:comp}
Simply deriving the equations for the PDF is not enough.
The many body nature of the PDF equation make analytical solution of the structure from the PDF impossible.
Thus, the PDF must be computed from a structural candidates and compared against experimental results to evaluate the reliability of the model.

\subsection{HPC and GPUs}
To properly solve the structure of materials the PDF will need to be computed many times and checked against experimental results.
This requires computation of the PDF, potentially over many atoms.
Calculating these PDFs requires a fast, highly parallized, computational framework.
\subsubsection{GPUs and Parallelization}
\begin{figure}
    \includegraphics[width=\textwidth]{cpu_gpu}
    \caption[Comparison of the CPU and GPU chip architectures]{
    Comparison of the CPU and GPU chip architectures from \cite{Chen2013}.
    The ALU are the arithmetic logic units which perform the matematical operations,
    the DRAM holds most of the data, although it is slower to access,
    the Cache holds rapidly accessed data,
    and the Control controls the exicution of software.
    Note the greater number of ALUs on the GPU, the comparitivly smaller cache,
    and the allocaiton of caches and controls to entire rows of processors.}
    \label{fig:cpu_vs_gpu}
\end{figure}
Computing the PDF is an embarrassingly parallel problem.
The basic procedure is to calculate the reduced structure factor $F(Q)$ for each atom pair and momentum transfer vector, sum over all the atom pairs, and Fourier transform the structure to the PDF.
The first part of this procedure is perfectly parallizable, as each atom pair is seperate from the others.
The summation over all the atomic reduced structure factors can be parallelized via distributed summing.
Lastly the FFT can be parallelized using existing parellel FFT algorithms.

GPUs are particularly well suted to the task of computing PDFs.
GPU chip architecture is designed to perform many task simultaniously by having potentially thousands of cores.
Figure \ref{fig:cpu_vs_gpu} show the comparison of CPU and GPU architectures.
As the figure shows the GPUs have a very different layout of computational processors (ALUs) and memory.
While each ALU is simpler on the GPU, requireing the instructions to be less demanding in terms of memory, there are many more of them.
The greater number of processors allows each atomic pairing to be placed on its own processor, so long as the math can be broken into simpler operations.
The equations are broken up on the gpus into various pieces which correspond to the $\alpha, \beta, \uptau$ and $\Omega$ as shown in equations \ref{eq:alpha}-\ref{eq:omega} and subequations as needed.
For example, while $\beta$ is computed in one step, $\Omega$ requires the computation of the displacement array, then the distance array and finally the $\Omega$ array.
The exact breakdown of processies, how the problems are broken down and spread across the processor has been optimized for speed and reliability.

\subsubsection{Map from ij space to k space}
The above equations, although formally correct, are very ineffiecent. $F(Q)$ and its gradient are indexed over all the atoms twice, however there are symmetries that allow us to only compute over the atom pairs esentially mapping from an $n$x$n$ space, $ij$ space, to a $\frac{n(n-1)}{2}$ space, $k$ space.
For $F(Q)$ we apply the following mapping
\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
    \node (E) at (0,0) {$E$};
    \node[right=of E] (F) {$E'$};
    \node[right=of F] (Z) {$Z$};
    \node[below=of F] (N) {$B'$};
    \node[below=of E] (M) {$B$};
    \draw[->] (E)--(F) node [midway,above] {$\psi$};
    \draw[->] (F)--(Z) node [midway,above] {$\Sigma$};
    \draw[->] (M)--(N) node [midway,below] {$\psi'$};
    \draw[->] (E)--(M) node [midway,left] {$\phi$};
    \draw[->] (N)--(Z) node [midway,left] {$\Sigma'$};
\end{tikzpicture}
\end{center}
\end{figure}
where $E$ denotes the atomic coordinates in $ij$ space, $E'$ denotes $F(Q)$ before the summation in $ij$ space, $B$ denotes the atomic pairs in $k$ space, $B'$ denotes $F(Q)$ in $k$ space, and $Z$ denotes the final summed $F(Q)$.  For the operators, $\phi$ denotes the mapping from $ij$ space to $k$ space $k = j + i * \frac{i - 1}{2}$, $\psi$ and $\psi'$ denote the $F(Q)$ operation in $ij$ and $k$ space, respectivly. $\Sigma$ denotes the sum over all the atoms.  

To properly define $\Sigma'$ we must establish whether $F(Q)$ is an even function.  
We can accomplish this by examining each of the portions of $F(Q)$, $\alpha, \beta ,\uptau, \Omega$.
$\Omega$ is even, since $r_{ij}$ is the interatomic distance, which is the same despite a flip of indicies, $Q$ does not depend on the atomic indicies, and since $Qr_{ij}$ is even so is $\sin{Qr_{ij}}$.  Thus, $\Omega$ is even.  Providing similar analysis to $\uptau$ we can see that while $\vec{u}_{ij}$ is odd, so is the unit displacement vector between the two atoms, thus the two odds cancel out.
Intuitivly this makes sense, since the $F(Q)$ equation is fundamentally interested in the interatomic distances which is even.  Thus, switching atom indicies does not change $F(Q)$.
Due to the even nature of the $F(Q)$ operator the $\Sigma'$ operator sums over all the atom pairs, and multiplies by two to reflect the double counting of the $\Sigma$ operator.

For the gradient a similar mapping is used:
\begin{figure}[h!]
\begin{center}
  \begin{tikzpicture}
    \node (E) at (0,0) {$E$};
    \node[right=of E] (F) {$E'$};
    \node[right=of F] (Z) {$Z$};
    \node[below=of F] (N) {$B'$};
    \node[below=of E] (M) {$B$};
    \draw[->] (E)--(F) node [midway,above] {$\psi$};
    \draw[->] (F)--(Z) node [midway,above] {$\Sigma$};
    \draw[->] (M)--(N) node [midway,below] {$\psi'$};
    \draw[->] (E)--(M) node [midway,left] {$\phi$};
    \draw[->] (N)--(Z) node [midway,left] {$\tilde{\phi}\Sigma$};
\end{tikzpicture}
\end{center}
\end{figure}

In this mapping, however, we use the $\tilde{\phi}\Sigma$ operator.  This operator simultaniously performs a reverse mapping from $k$ to $ij$ space, and a summation with the correct symmetry.  In this case the $\psi$ and $\psi'$ operators, which denote the $\grad{F(Q)}$ operator in $ij$ and $k$ space, are antisymmetric.  Intuitivly this makes sense as an extension of Newton's Second Law, since each particle's interation is felt oppositely by its partner.
\subsubsection{GPU Memory Allocation}
While GPUs are very fast computational engines they tend to be memory bound.
While a gradient array for a 10nm Au nanoparticle, consisting of 31,000 atoms and half a billion unique distances, occupies 1.5 TB of memory a single GPU's RAM allotment varies from 4GB on a NVIDIA GTX970 to 24 GB on a NVIDIA Tesla K80.
Thus, it is important to determine exactly how many atoms can fit on a GPU of arbitrary size as a funciton of the number of atoms and the $Q$ range.
The memory required per array is:
\begin{eqnarray}
    q [=] 3n\\
    d [=] 3k\\
    r [=] k\\
    scatter [=] nQ\\
    normalization [=] kQ\\
    \Omega [=] kQ\\
    F_{k}(Q) [=] kQ\\
    Sum [=] kQ\\
    Sum2 [=] kQ\\
    F(Q) [=] Q
\end{eqnarray}
where $n$ is the number of atoms, $k$ is the number of uniqe distances, $Q$ is the scatter vector, and the $[=]$ operator denote the number of single percision floating point values in memory.
Each of the above arrays are used in the computation and thus must be able to be held in memory.
Thus the number of atom pairs that can fit on a GPU with $am$ bytes of available memory is:
\begin{equation}
    k_{per GPU} = \frac{1}{16 Q + 16} \left(- 4 Q n - 4 Q + am - 12 n\right)
\end{equation}
If ADPs ar eincluded in the calculation, then the following arrays are also added to the memory allocaiton:
\begin{eqnarray}
    adps = 3n\\
    \sigma = k\\
    \uptau = kQ
\end{eqnarray}
Thus the pair allotment is:
\begin{equation}
    k_{per GPU} = \frac{- 4 Q n - 4 Q + am - 24 n}{20 Q + 20}
\end{equation}

For the Gradient we need to calculate $F(Q)$ and its gradient, so the total memeory overhead is equal to the previously mentioned arrays plus:
\begin{eqnarray}
    \grad{\Omega} = 3kQ \\
    \grad{F_{k}(Q)} = 3kQ \\
    \grad{F_{n}(Q)} = 3nQ
\end{eqnarray}
Thus the gradient allotment is:
\begin{equation}
    \grad{k_{per GPU}} = \frac{- 16 Q n + am - 12 n}{32 Q + 16}
\end{equation}
For the gradient with ADPs the ADP gradient array is:
\begin{equation}
    \grad{\uptau} = 3kQ
\end{equation}
Thus the allocation is:
\begin{equation}
    \grad{k_{per GPU}} = \frac{ - 16 Q n + am - 24 n}{48 Q + 20}
\end{equation}
These equations were solved by sympy as their validity is very important to the overal relyability of the software.
If the GPU is overallocated then the system may crash or return meaningless results.

\subsection{Speed and Scaling of PDF Computation}
To understand exactly how much the GPUs speed up the computation of $F(Q)$ and the PDF a series of time studies were run Au nanoparticles of varying size.
Figure \ref{fig:speed} shows the results of these time studies. CPU and GPU calculations were carried out on an Intel i7-4820K $@$3.70GHz Quad-Core and one Nvidia GTX970s, respectively.
\begin{figure}
    \foreach \n in {fq_log, speed_log}{
        \subfloat[]{\includegraphics[width=.5\textwidth]{\n}}
        }
    \caption[Speed comparison of CPU and GPU implementations]{Speed comparison of CPU and GPU implementations.
    a) shows the time to compute the $F(Q)$ by itself.
    b) shows the time to compute the Rw based energy for Au NPs of various sizes, which includes computing $F(Q)$, its FFT, and the $Rw.$}
    \label{fig:speed}
\end{figure}
The $F(Q)$ computations show a 100x to 10x speedup using the GPUs over the CPUs.
Additionally, the $\grad{F(Q)}$ and $F(Q)$ computations seem to have similar computation time and scaling relationships on the GPU.
This implies that the two proceisses may have similar bottlenecks, most likely in the $F(Q)$ computation workflow.
This relationship is similarly preserved, although to a lesser extent, in the CPU scaling.

Interestingly, the tight runtime relationship between $F(Q)$ and its gradient are not preserved in the $Rw$ based force calculations.
While the energy calculations are very similar to the $F(Q)$ calcultions in terms of runtime, the GPU and CPU force calculations are much closer, with the GPU calculations being much slower.
This is due to the force bottleneck being the $3n$ FFT operations which must be performed on the $\grad{F(Q)}$ array to produce the $\grad{PDF}$ array.
While the GPU is leveraged to perform the FFT, the data must be loaded off the GPU and back on, causing a potential slowdown.
Larger systems of atoms were not tried as the CPU computation quickly becomes very slow.
Even higher GPU  speedup is expected on more advanced GPUs like the Nvidia Tesla series.
